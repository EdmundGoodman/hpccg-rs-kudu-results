run_configurations:
  "cpp-openmp":
    sbatch_config:
      "nodes": 1
      "ntasks-per-node": 1
      "cpus-per-task": 32
      "exclusive": "mcs"
      "mem": 40000
    module_loads: []
    environment_variables: {}
    directory: "./generated_results/hpccg-rs-kudu-results/hpccg-rs/0_cpp_versions/1_openmp"
    build_commands:
      - "make no_yaml"
      - "make -j 8"
    run_command: "./test_HPCCG"

  "rust-rayon":
    sbatch_config:
      "nodes": 1
      "ntasks-per-node": 1
      "cpus-per-task": 32
      "exclusive": "mcs"
      "mem": 40000
    module_loads: []
    environment_variables: {}
    directory: "./generated_results/hpccg-rs-kudu-results/hpccg-rs/6_parallel"
    build_commands:
      - "cargo build --release"
    run_command: "./target/release/hpccg-rs"

  "cpp-mpi":
    sbatch_config:
      "exclusive": "mcs"
      "mem": 40000
    module_loads:
      - "cs402-mpi"
    environment_variables: {}
    directory: "./generated_results/hpccg-rs-kudu-results/hpccg-rs/0_cpp_versions/2_mpi"
    build_commands:
      - "make no_yaml"
      - "make -j 8"
    run_command: "mpirun ./test_HPCCG"

  "rust-mpi":
    sbatch_config:
      "exclusive": "mcs"
      "mem": 40000
    module_loads:
      - "cs402-mpi"
    environment_variables: {}
    directory: "./generated_results/hpccg-rs-kudu-results/hpccg-rs/7_mpi"
    build_commands:
      - "cargo build --release"
    run_command: "mpirun ./target/release/hpccg-rs"


benches:
  "strong-scaling-threaded":
    run_configurations:
      - "cpp-openmp"
      - "rust-rayon"
    reruns:
      number: 5
      highest_discard: 1
      unaggregatable_metrics:
        - "Mesh z size"
        - "Total FLOPs"
        - "Parallelisation degree"
    matrix:
      [args, environment_variables]:
        - ["64 64 1024", { "OMP_NUM_THREADS": 1, "RAYON_NUM_THREADS": 1 }]
        - ["64 64 512", { "OMP_NUM_THREADS": 2, "RAYON_NUM_THREADS": 2 }]
        - ["64 64 256", { "OMP_NUM_THREADS": 4, "RAYON_NUM_THREADS": 4 }]
        - ["64 64 128", { "OMP_NUM_THREADS": 8, "RAYON_NUM_THREADS": 8 }]
        - ["64 64 64", { "OMP_NUM_THREADS": 16, "RAYON_NUM_THREADS": 16 }]
        - ["64 64 32", { "OMP_NUM_THREADS": 32, "RAYON_NUM_THREADS": 32 }]
    analysis:
      metrics:
        "Mesh z size": "nz: (\\d+)"
        "Wall time (s)": "real\\s([\\d\\.]+)\nuser"
        "Total time (s)": "Time Summary:[\\s\\S]*Total\\s*: ([\\d\\.]+)[\\s\\S]*\nFLOPS Summary"
        "Total FLOPs": "FLOPS Summary:[\\s\\S]*Total\\s*: ([\\d\\.eE\\-+]+)[\\s\\S]*\nMFLOPS Summary"
        "Total MFLOPs/s": "MFLOPS Summary:[\\s\\S]*Total\\s*: ([\\d\\.]+)"
        "Parallelisation degree": "=== RUN INSTANTIATION ===\n\\{.*environment_variables: \\{.*OMP_NUM_THREADS: (\\d+).*\\}"
      line_plots:
        - title: "Multi-threaded Strong Scaling Plot"
          x: "Parallelisation degree"
          y: "Wall time (s)"

  "weak-scaling-threaded":
    run_configurations:
      - "cpp-openmp"
      - "rust-rayon"
    reruns:
      number: 5
      highest_discard: 1
      unaggregatable_metrics:
        - "Mesh x size"
        - "Total FLOPs"
        - "Parallelisation degree"
    matrix:
      args:
        - "64 64 64"
        - "200 200 200"
      environment_variables:
        - { "OMP_NUM_THREADS": 1, "RAYON_NUM_THREADS": 1 }
        - { "OMP_NUM_THREADS": 2, "RAYON_NUM_THREADS": 2 }
        - { "OMP_NUM_THREADS": 4, "RAYON_NUM_THREADS": 4 }
        - { "OMP_NUM_THREADS": 8, "RAYON_NUM_THREADS": 8 }
        - { "OMP_NUM_THREADS": 16, "RAYON_NUM_THREADS": 16 }
        - { "OMP_NUM_THREADS": 32, "RAYON_NUM_THREADS": 32 }
    analysis:
      metrics:
        "Mesh x size": "nx: (\\d+)"
        "Wall time (s)": "real\\s([\\d\\.]+)\nuser"
        "Total time (s)": "Time Summary:[\\s\\S]*Total\\s*: ([\\d\\.]+)[\\s\\S]*\nFLOPS Summary"
        "Total FLOPs": "FLOPS Summary:[\\s\\S]*Total\\s*: ([\\d\\.eE\\-+]+)[\\s\\S]*\nMFLOPS Summary"
        "Total MFLOPs/s": "MFLOPS Summary:[\\s\\S]*Total\\s*: ([\\d\\.]+)"
        "Parallelisation degree": "=== RUN INSTANTIATION ===\n\\{.*environment_variables: \\{.*OMP_NUM_THREADS: (\\d+).*\\}"
      line_plots:
        - title: "Multi-threaded Weak Scaling Plot"
          x: "Parallelisation degree"
          y: "Wall time (s)"
          fix_metrics:
            "Mesh x size": 64
        - title: "Multi-threaded Weak Scaling Plot"
          x: "Parallelisation degree"
          y: "Wall time (s)"
          fix_metrics:
            "Mesh x size": 200




  "strong-scaling-message-passing":
    run_configurations:
      - "cpp-mpi"
      - "rust-mpi"
    reruns:
      number: 5
      highest_discard: 1
      unaggregatable_metrics:
        - "Mesh z size"
        - "Total FLOPs"
        - "MPI Ranks"
        - "MPI Nodes"
    matrix:
      [args, sbatch_config]:
        - ["64 64 1024", { "ntasks": 1, "nodes": 1 }]
        - ["64 64 512", { "ntasks": 2, "nodes": 1 }]
        - ["64 64 256", { "ntasks": 4, "nodes": 1 }]
        - ["64 64 128", { "ntasks": 8, "nodes": 2 }]
        - ["64 64 64", { "ntasks": 16, "nodes": 2 }]
    analysis:
      metrics:
        "Mesh z size": "nz: (\\d+)"
        "Wall time (s)": "real\\s([\\d\\.]+)\nuser"
        "Total time (s)": "Time Summary:[\\s\\S]*Total\\s*: ([\\d\\.]+)[\\s\\S]*\nFLOPS Summary"
        "Total FLOPs": "FLOPS Summary:[\\s\\S]*Total\\s*: ([\\d\\.eE\\-+]+)[\\s\\S]*\nMFLOPS Summary"
        "Total MFLOPs/s": "MFLOPS Summary:[\\s\\S]*Total\\s*: ([\\d\\.]+)"
        "Parallelisation degree": "=== RUN INSTANTIATION ===\n\\{.*sbatch_config: \\{.*ntasks: (\\d+).*\\}"
        "MPI Rank": "Parallelism:[\\s\\S]*Number of MPI ranks: (\\d*)"
        "MPI Nodes": "=== SLURM CONFIG ===[\\s\\S]*\ \ \ NumNodes=(\\d*)\ NumCPUs"
      line_plots:
        - title: "Message Passing Strong Scaling Plot"
          x: "Parallelisation degree"
          y: "Wall time (s)"

  "weak-scaling-message-passing":
    run_configurations:
      - "cpp-mpi"
      - "rust-mpi"
    reruns:
      number: 5
      highest_discard: 1
      unaggregatable_metrics:
        - "Mesh x size"
        - "Total FLOPs"
        - "MPI Ranks"
        - "MPI Nodes"
    matrix:
      args:
        - "64 64 64"
        - "200 200 200"
      sbatch_config:
        - { "ntasks": 1, "nodes": 1 }
        - { "ntasks": 2, "nodes": 1 }
        - { "ntasks": 4, "nodes": 1 }
        - { "ntasks": 8, "nodes": 2 }
        - { "ntasks": 16, "nodes": 2 }
    analysis:
      metrics:
        "Mesh x size": "nx: (\\d+)"
        "Wall time (s)": "real\\s([\\d\\.]+)\nuser"
        "Total time (s)": "Time Summary:[\\s\\S]*Total\\s*: ([\\d\\.]+)[\\s\\S]*\nFLOPS Summary"
        "Total FLOPs": "FLOPS Summary:[\\s\\S]*Total\\s*: ([\\d\\.eE\\-+]+)[\\s\\S]*\nMFLOPS Summary"
        "Total MFLOPs/s": "MFLOPS Summary:[\\s\\S]*Total\\s*: ([\\d\\.]+)"
        "Parallelisation degree": "=== RUN INSTANTIATION ===\n\\{.*sbatch_config: \\{.*ntasks: (\\d+).*\\}"
        "MPI Rank": "Parallelism:[\\s\\S]*Number of MPI ranks: (\\d*)"
        "MPI Nodes": "=== SLURM CONFIG ===[\\s\\S]*\ \ \ NumNodes=(\\d*)\ NumCPUs"
      line_plots:
        - title: "Message Passing Weak Scaling Plot"
          x: "Parallelisation degree"
          y: "Wall time (s)"
          fix_metrics:
            "Mesh x size": 64
        - title: "Message Passing Weak Scaling Plot"
          x: "Parallelisation degree"
          y: "Wall time (s)"
          fix_metrics:
            "Mesh x size": 200
